{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3a3f83",
   "metadata": {},
   "source": [
    "# HISevent processing and storage script\n",
    "\n",
    "The goal of this script is to process incoming HISevent files from a HISevent folder, into clean data to be stored in an SQL Server database. SQLite was not chosen due to connection issues with Power BI.\n",
    "\n",
    "**Workflow Overview:**\n",
    "1. Initialise Python environment\n",
    "2. Initialise directory\n",
    "3. Initialise connection to SQL Server database (Database and tables created separately)\n",
    "4. Initiate Processing Loop Sequence\n",
    "4.1. Read in oldest raw HISevent log file (target file) from target folder (Raw File Source)\n",
    "4.2. Clean and Process raw HISevent \n",
    "4.3. Append cleaned data into SQL Server database table\n",
    "4.4. Delete target file from target folder\n",
    "4.5. Save cleaned data as a new CSV in a separate folder (Optional Step)\n",
    "5. End loop when there are no more files in folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd819b8e",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fbbe7",
   "metadata": {},
   "source": [
    "### Initialise Python Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b96927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\Python_3-9-master\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Check python environment\n",
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a40f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import datetime as dt\n",
    "import time\n",
    "import memory_profiler # for logging memory consumption at the function level\n",
    "import psutil # or logging memory consumption at that point instant\n",
    "import os # for manipulating file directories\n",
    "import pyodbc # for sql operations\n",
    "import sqlalchemy # for sql operations with pandas\n",
    "import urllib # for defining sql connection parameters\n",
    "import shutil # for transferring files between folders\n",
    "#import swifter # for speeding up pandas apply functions on vectorised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc892161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load memory profiler to log memory usage by function\n",
    "# uses the \"%memit\" prefix before each function to start logging\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244bda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.55859375\n",
      "102.55859375\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "p = psutil.Process()\n",
    "# Get peak memory usage at that instant of time\n",
    "print(p.memory_info().peak_wset / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150ef35a",
   "metadata": {},
   "source": [
    "### Configure display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2713f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable display of all columns for dataframes with many variables\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d95e3",
   "metadata": {},
   "source": [
    "### Initialise Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c35d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current directory location\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c39900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root file directory folder where the files are being stored\n",
    "#os.chdir(cwd + alarmLoc)\n",
    "os.chdir(os.path.dirname(os.getcwd()) + '\\\\alarm-event-logs')\n",
    "\n",
    "# Check current directory location\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Check directory location\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef62092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of source files\n",
    "srcLoc = '\\\\HISevent_data\\\\Logs_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b2aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root file directory folder where the files are being stored\n",
    "os.chdir(cwd + srcLoc)\n",
    "#os.chdir(cwd + eventLoc)\n",
    "# Check directory location\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c03f86",
   "metadata": {},
   "source": [
    "### Initialise SQL Server Database\n",
    "To Store cleaned log files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a469ec",
   "metadata": {},
   "source": [
    "#### Connect to SQL Server Database & Define Core Functions\n",
    "Actual database creation and table creation is done on Microsoft SQL Management Studio. Remember to Close Connection when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac473b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose to activate Test Mode or Not\n",
    "testMode = True\n",
    "\n",
    "# Choose whether to reset database table (clear all data)\n",
    "resetTableData = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6069fe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pyodbc version\n",
    "print(pyodbc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef1523e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find available server drivers\n",
    "pyodbc.drivers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43514cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a database connection to target database\n",
    "# All subsequent functions will depend on this connection\n",
    "# Remember to close connection when done\n",
    "conn = pyodbc.connect('Driver={SQL Server};'\n",
    "                      'Server=SBSR-RD-0K00200;'\n",
    "                      'Database=HISevent_DBtest;'\n",
    "                      'Trusted_Connection=yes;')\n",
    "\n",
    "# Define Server Parameters to Initiate Connection Engine via SQL Alchemy\n",
    "# This has the same values as the connection request\n",
    "# Only if one uses the windows authentification method\n",
    "# Otherwise, one will need to define \"UID\" (user ID) + \"PWD\" (password)\n",
    "serverParams = urllib.parse.quote_plus('Driver={SQL Server};'\n",
    "                                       'Server=SBSR-RD-0K00200;'\n",
    "                                       'Database=HISevent_DBtest;'\n",
    "                                       'Trusted_Connection=yes;'\n",
    "                                       #\"UID=user;\"\n",
    "                                       #\"PWD=password\"\n",
    "                                      )\n",
    "\n",
    "# Create cursor to work in database\n",
    "# SQL auto commits transactions\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Inspect All Tables\n",
    "def list_dbTables():\n",
    "    # Check list of tables in db\n",
    "    cursor.execute(\"SELECT table_name FROM INFORMATION_SCHEMA.TABLES \")\n",
    "    print(cursor.fetchall())\n",
    "\n",
    "# List tables in database\n",
    "# Note: %memit\" prefix is used to log peak memory uage\n",
    "%memit list_dbTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list contents in target table\n",
    "def list_tableContents(targetTable, rowLimit = 10):\n",
    "    # Gets first n rows (rowLimit) from target table sorted by datetime (oldest entry first)\n",
    "    # Not allowed to get all values as the table size can be huge\n",
    "    cursor.execute(f\"SELECT top {rowLimit} * FROM {targetTable} ORDER BY 'DATEANDTIME' ASC\")\n",
    "    results = cursor.fetchall()\n",
    "    print(targetTable, \"Contents\")\n",
    "    counter = 0\n",
    "    print(\"Table Values\")\n",
    "    print(\"---START---\")\n",
    "    for row in results:\n",
    "        counter=counter+1\n",
    "        print(counter, row)\n",
    "    print(\"---END---\")\n",
    "\n",
    "# Define the target table\n",
    "# This table will be where the cleaned data would be saved to \n",
    "# and manipulated for future operations\n",
    "targetTable = \"TestLog_cleaned\"\n",
    "\n",
    "# Inspect target table\n",
    "%memit list_tableContents(targetTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de10a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to list contents in target table\n",
    "def list_tableColDtype(targetTable):\n",
    "    cursor.execute(f\"SELECT COLUMN_NAME, DATA_TYPE FROM information_schema.columns where TABLE_NAME = '{targetTable}'\")\n",
    "    results = cursor.fetchall()\n",
    "    print(targetTable, \"Contents\")\n",
    "    counter = 0\n",
    "    print(\"Table Column Data Type\")\n",
    "    print(\"---START---\")\n",
    "    for row in results:\n",
    "        counter=counter+1\n",
    "        print(counter, row)\n",
    "    print(\"---END---\")\n",
    "\n",
    "# Inspect target table\n",
    "# This will get the firs\n",
    "targetTable = \"TestLog_cleaned\"\n",
    "%memit list_tableColDtype(targetTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23885107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create a dataframe with dummy data for testing purposes\n",
    "def createDummyDataDF():\n",
    "    dummyData = {\n",
    "                 'ALARMID': [1, 2, 3],\n",
    "                 'ENVIRONMENT': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'VALUE': [1, 2, 3],\n",
    "                 'ACKREQUIRED': [1, 2, 3],\n",
    "                 'SEVERITY': [1, 2, 3],\n",
    "                 'EQUIPMENTCLASS': [1, 2, 3],\n",
    "                 'FUNCTIONALCAT': [1, 2, 3],\n",
    "                 'GEOGRAPHICALCAT': [1, 2, 3],\n",
    "                 'DATEANDTIME': [1644223829, 1644310229, 1644396629],\n",
    "                 'EQUIPMENTNAME': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'ASSETNAME': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'MESSAGE': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'STATUS': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'GROUP1': [1, 2, 3],\n",
    "                 'GROUP2': [1, 2, 3],\n",
    "                 'FORMAT': [1, 2, 3],\n",
    "                 'DSSEVENTTYPE': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'OPER': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'ASSET_DESC_CAT': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'EVENT_DESC_CAT': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'TrainID': [1, 2, 3],\n",
    "                 'CarID': [1, 2, 3],\n",
    "                 'ServiceID': [1, 2, 3],\n",
    "                 'AssetClass': ['Alpha', 'Bravo', 'Charlie'],\n",
    "                 'AssetSubClass': ['Alpha', 'Bravo', 'Charlie'] \n",
    "                }\n",
    "\n",
    "    df = pd.DataFrame(dummyData)\n",
    "    df['DATEANDTIME'] = pd.to_datetime(df['DATEANDTIME'], unit='s')\n",
    "    df['DATE'] = df['DATEANDTIME'].dt.date\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['TIME'] = df['DATEANDTIME'].dt.time\n",
    "    # To get time in seconds resolution if it comes in higher resolutions\n",
    "    # Not required\n",
    "    #df['TIME_S'] = df['DATEANDTIME'].dt.floor(\"s\").dt.time\n",
    "\n",
    "    return df\n",
    "\n",
    "# Generate test dataframe in testing mode\n",
    "if (testMode == True):\n",
    "    # Create a dummy dataframe for testing purposes\n",
    "    %memit testDF = createDummyDataDF()\n",
    "    print(\"Dataframe columns would have the same data type as SQL table values\")\n",
    "    testDF.info()\n",
    "else:\n",
    "    print(\"Test script skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect test dataframe\n",
    "if (testMode == True):\n",
    "    # Inspect Data\n",
    "    print(testDF.head())\n",
    "else:\n",
    "    print(\"Test script skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append data to database as an entire dataframe\n",
    "def appendData(tableName, inputDF, serverParams):    \n",
    "    # Create connection engine \n",
    "    # Default connection function only works for SQLite\n",
    "    engine = sqlalchemy.create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(serverParams))\n",
    "    # Append data\n",
    "    inputDF.to_sql(tableName, con=engine, if_exists=\"append\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239efeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data append function\n",
    "if (testMode == True):\n",
    "    # Test if append dataframe function works\n",
    "    %memit appendData(\"TestLog_cleaned\", testDF, serverParams)\n",
    "    %memit list_tableContents(\"TestLog_cleaned\")\n",
    "else:\n",
    "    print(\"Test script skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8635c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to delete last n rows in table sorted by datetime (oldest first)\n",
    "def delDataNRow(targetTable, nRow=3):\n",
    "    cursor.execute(f\"WITH CTE AS (SELECT TOP {nRow} * FROM {targetTable} ORDER BY DATEANDTIME DESC) DELETE FROM CTE\")\n",
    "    conn.commit()\n",
    "    \n",
    "# Function to delete all rows in table\n",
    "def delDataAll(targetTable):\n",
    "    cursor.execute(f\"DELETE FROM {targetTable}\")\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee607b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data delete latest nRows function\n",
    "if (testMode == True):\n",
    "    %memit delDataNRow(targetTable, nRow=2)\n",
    "    %memit list_tableContents(targetTable)\n",
    "else:\n",
    "    print(\"Test script skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbf10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data delete all function\n",
    "if (resetTableData == True):\n",
    "    %memit delDataAll(targetTable)\n",
    "    %memit list_tableContents(targetTable)\n",
    "else:\n",
    "    print(\"Test script skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dce692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant variables used in test\n",
    "if (resetTableData == True):\n",
    "    del testDF\n",
    "else:\n",
    "    print(\"Test script skipped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b4e2a",
   "metadata": {},
   "source": [
    "## Loop to Process Log Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406103a0",
   "metadata": {},
   "source": [
    "### Define Dependent Functions to Process Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ad2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to transfer files which has been processed successfully to an archive folder\n",
    "def fileTransfer(file_name, dst_folder, src_folder=\"\"):\n",
    "    # If current folder is the source directory for the files, leave src_folder empty\n",
    "    # check if file exist in destination\n",
    "    if os.path.exists(dst_folder + file_name):\n",
    "        # Split name and file type extension\n",
    "        data = os.path.splitext(file_name)\n",
    "        only_name = data[0]\n",
    "        extension = data[1]\n",
    "        # Adding the new name\n",
    "        new_base = only_name + '_new' + extension\n",
    "        # construct full file path\n",
    "        new_name = os.path.join(dst_folder, new_base)\n",
    "        # move file\n",
    "        shutil.move(src_folder + file_name, new_name)\n",
    "    else:\n",
    "        shutil.move(src_folder + file_name, dst_folder + file_name)\n",
    "    print(file_name, \"Transferred to destination folder\", dst_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0069c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileIngestor(inputFile):\n",
    "    \n",
    "    # Note read_csv cannot be used due to some level of file corruption in the logs\n",
    "    # Hence, a more complicated data ingestion method is required\n",
    "    \n",
    "    # Read in file\n",
    "    file = open(inputFile, 'r')\n",
    "    # Convert file contents to a list\n",
    "    fileContents = list(file)\n",
    "    # Close file\n",
    "    file.close()\n",
    "\n",
    "    # Load File Data as a Dataframe\n",
    "    df = pd.DataFrame(fileContents,columns=['rawData'])\n",
    "\n",
    "    # Drop non-relevant rows and reset index\n",
    "    df = df.drop([0,1], axis=0).reset_index().drop([\"index\"], axis=1)\n",
    "\n",
    "    # Define Header Names\n",
    "    headerList_core = [\n",
    "                    \"ALARMID\",\n",
    "                    \"ENVIRONMENT\",\n",
    "                    \"VALUE\",\n",
    "                    \"ACKREQUIRED\",\n",
    "                    \"SEVERITY\",\n",
    "                    \"EQUIPMENTCLASS\",\n",
    "                    \"FUNCTIONALCAT\",\n",
    "                    \"GEOGRAPHICALCAT\",\n",
    "                    \"DATEANDTIME\",\n",
    "                    \"EQUIPMENTNAME\",\n",
    "                    \"ASSETNAME\",\n",
    "                    \"MESSAGE\",\n",
    "                    \"STATUS\",\n",
    "                    \"GROUP1\",\n",
    "                    \"GROUP2\",\n",
    "                    \"FORMAT\",\n",
    "                    \"DSSEVENTTYPE\",\n",
    "                    \"OPER\",\n",
    "                    \"UnknownVariable\"\n",
    "                ]\n",
    "    df = df[\"rawData\"].str.split(pat=\";\", n=18, expand=True).rename(columns={0: headerList_core[0],\n",
    "                                                                              1: headerList_core[1],\n",
    "                                                                              2: headerList_core[2],\n",
    "                                                                              3: headerList_core[3],\n",
    "                                                                              4: headerList_core[4],\n",
    "                                                                              5: headerList_core[5],\n",
    "                                                                              6: headerList_core[6],\n",
    "                                                                              7: headerList_core[7],\n",
    "                                                                              8: headerList_core[8],\n",
    "                                                                              9: headerList_core[9],\n",
    "                                                                             10: headerList_core[10],\n",
    "                                                                             11: headerList_core[11],\n",
    "                                                                             12: headerList_core[12],\n",
    "                                                                             13: headerList_core[13],\n",
    "                                                                             14: headerList_core[14],\n",
    "                                                                             15: headerList_core[15],\n",
    "                                                                             16: headerList_core[16],\n",
    "                                                                             17: headerList_core[17],\n",
    "                                                                             18: headerList_core[18]})\n",
    "    del df[\"UnknownVariable\"]\n",
    "    \n",
    "    # Return ingested file as a dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc82c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDataAnomalies(dfInput):\n",
    "    # Clean up exception cases\n",
    "    dfInput.loc[df[\"EQUIPMENTNAME\"].str.contains(\"\\n\", na = False, regex = False), \"EQUIPMENTNAME\"] = \"\"\n",
    "    dfInput[\"EQUIPMENTNAME\"] = dfInput[\"EQUIPMENTNAME\"].str.replace(\"\b\", \"\", regex = False)\n",
    "    dfInput[\"ASSETNAME\"] = dfInput[\"ASSETNAME\"].str.replace(\"\b\", \"\", regex = False)\n",
    "    dfInput[\"MESSAGE\"] = dfInput[\"MESSAGE\"].str.replace(\"\b\", \"\", regex = False)\n",
    "    dfInput[\"EQUIPMENTNAME\"] = dfInput[\"EQUIPMENTNAME\"].str.replace(\"?\", \"\", regex = False)\n",
    "    dfInput[\"ASSETNAME\"] = dfInput[\"ASSETNAME\"].str.replace(\"?\", \"\", regex = False)\n",
    "    dfInput[\"MESSAGE\"] = dfInput[\"MESSAGE\"].str.replace(\"?\", \"\", regex = False)\n",
    "\n",
    "    dfInput.loc[df[\"DATEANDTIME\"] == \"SKY\", [\"DATEANDTIME\", \"OPER\"]] = None, \"SKY\"\n",
    "    dfInput.loc[df[\"ACKREQUIRED\"] == 1, \"ACKREQUIRED\"] = True\n",
    "    dfInput.loc[df[\"ACKREQUIRED\"] == 0, \"ACKREQUIRED\"] = False\n",
    "    dfInput.loc[(df[\"OPER\"] == \"\\x18\\x1a\\x11\") | \n",
    "           (dfInput[\"OPER\"] == \"(^+\") | \n",
    "           (dfInput[\"OPER\"] == \"\\x18*V\") | \n",
    "           (dfInput[\"OPER\"] == \"\\x18\\x1aM\") |\n",
    "           (dfInput[\"OPER\"] == \"\\x18Z\\x0c\") |\n",
    "           (dfInput[\"OPER\"] == \"\\x18JI\") |\n",
    "           (dfInput[\"OPER\"] == \"\\x18\\n\") |\n",
    "           (dfInput[\"OPER\"] == \"\\x18j\\x14\") |\n",
    "           (dfInput[\"OPER\"] == \"\\x18:H\"), \"OPER\"] = None\n",
    "\n",
    "    # Delete corrupted rows\n",
    "    dfInput = dfInput.drop(dfInput[dfInput[\"ENVIRONMENT\"].str.contains(\"TRACTION\", na = False, regex = False) | \n",
    "                    (dfInput[\"ENVIRONMENT\"] == \"Executed\")].index)\n",
    "    dfInput = dfInput.drop(dfInput[(dfInput[\"ENVIRONMENT\"] == \"\\n\")].index)\n",
    "\n",
    "    # Clean up partially corrupted rows\n",
    "    dfInput.loc[(dfInput[\"GROUP1\"] == \"NORMAL\"),\n",
    "          [\"EQUIPMENTNAME\",\n",
    "           \"ASSETNAME\",\n",
    "           \"MESSAGE\",\n",
    "           \"STATUS\",\n",
    "           \"GROUP1\",\n",
    "           \"GROUP2\",\n",
    "           \"FORMAT\",\n",
    "           \"DSSEVENTTYPE\" \n",
    "          ]] = dfInput.iloc [::, 9:17].shift(periods=-1, axis=\"columns\")\n",
    "    dfInput.loc[(df[\"GROUP1\"] == \"REQUESTED\") & \n",
    "           (dfInput[\"MESSAGE\"].str.contains(\"TRACTION\", na = False, regex = False)),\n",
    "          \"EQUIPMENTNAME\"] = dfInput[\"EQUIPMENTNAME\"] + dfInput[\"ASSETNAME\"]\n",
    "    dfInput.loc[(dfInput[\"GROUP1\"] == \"REQUESTED\") & \n",
    "           (dfInput[\"MESSAGE\"].str.contains(\"TRACTION\", na = False, regex = False)),\n",
    "          [\"ASSETNAME\", \n",
    "           \"MESSAGE\",\n",
    "           \"STATUS\",\n",
    "           \"GROUP1\",\n",
    "           \"GROUP2\",\n",
    "           \"FORMAT\",\n",
    "           \"DSSEVENTTYPE\" \n",
    "          ]] = dfInput.iloc [::, 10:17].shift(periods=-1, axis=\"columns\")\n",
    "\n",
    "    # Remove trailing non-printable characters\n",
    "    dfInput[\"ALARMID\"] = dfInput[\"ALARMID\"].str.strip()\n",
    "    dfInput[\"EQUIPMENTNAME\"] = dfInput[\"EQUIPMENTNAME\"].str.strip()\n",
    "    dfInput[\"MESSAGE\"] = dfInput[\"MESSAGE\"].str.strip()\n",
    "    dfInput[\"ASSETNAME\"] = dfInput[\"ASSETNAME\"].str.strip()\n",
    "    \n",
    "    # Return dataframe with data anomalies removed\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanLocNames(dfInput):\n",
    "    \n",
    "    locNamesList = {\n",
    "                    'NED', #001\n",
    "                    'FRP', #002\n",
    "                    'SKG', #003\n",
    "                    'HGN', #004\n",
    "                    'KVN', #005\n",
    "                    'SER', #006\n",
    "                    'HBF', #007\n",
    "                    'DBG', #008\n",
    "                    'OTP', #009\n",
    "                    'CNT', #010\n",
    "                    'LTI', #011\n",
    "                    'CQY', #012\n",
    "                    'BGK', #013\n",
    "                    'OCC', #014\n",
    "                    'WLH', #015\n",
    "                    'PTP', #016\n",
    "                    'BNK', #017\n",
    "                    'PGL', #018\n",
    "                    'TUNNEL', #019\n",
    "                    'Sector', #020\n",
    "                    'Concourse', #021\n",
    "                    'Mezzaninne', #022\n",
    "                    'Mid-Landing Entrance', #023\n",
    "                    'AL', #024\n",
    "                    'Dirty Area', #025\n",
    "                    'IAP', #026\n",
    "                    '1st Storey', #027\n",
    "                    '2nd Storey', #028\n",
    "                    '3rd Storey', #029\n",
    "                    'B1', #030\n",
    "                    'B2', #031\n",
    "                    'B3', #032\n",
    "                    'Entrance', #033\n",
    "                    'Mid Landing', #034\n",
    "                    'Mid-Landing', #035\n",
    "                    'Subway', #036\n",
    "                    'Underpass Link', #037\n",
    "                    \"Underpass To EXT'G  STN\", #038\n",
    "                    \"1st\", #039 NEW UPDATE\n",
    "                    \"2nd\", #040 NEW UPDATE\n",
    "                    \"SUBLOCATIONN\", #041 NEW UPDATE\n",
    "                    \"SUBLOCATIONS\", #042 NEW UPDATE\n",
    "                    \"North End\", #043 NEW UPDATE\n",
    "                    \"South End\", #044 NEW UPDATE\n",
    "                    \"South Adjacent\", #045 NEW UPDATE\n",
    "                    \"North Adjacent\", #046 NEW UPDATE\n",
    "                    \"Mezzanine\", #047 NEW UPDATE\n",
    "                    \"Linkway\", #048 NEW UPDATE\n",
    "                    \"Smoke Free Lobby\", #049 NEW UPDATE\n",
    "                    \"Storey\", #050 NEW UPDATE\n",
    "                    \"Underpass to EXT'G STN\", #051 NEW UPDATE\n",
    "                    \"-SUBLOCATION\", #052 NEW UPDATE\n",
    "                    \"SUBLOCATION-\" #053 NEW UPDATE\n",
    "\n",
    "                    }\n",
    "\n",
    "\n",
    "    locNamesVal = [\n",
    "                '', #001\n",
    "                '', #002\n",
    "                '', #003\n",
    "                '', #004\n",
    "                '', #005\n",
    "                '', #006\n",
    "                '', #007\n",
    "                '', #008\n",
    "                '', #009\n",
    "                '', #010\n",
    "                '', #011\n",
    "                '', #012\n",
    "                '', #013\n",
    "                '', #014\n",
    "                '', #015\n",
    "                '', #016\n",
    "                '', #017\n",
    "                '', #018\n",
    "                '', #019\n",
    "                '', #020\n",
    "                'SUBLOCATION', #021\n",
    "                'SUBLOCATION', #022\n",
    "                'SUBLOCATION', #023\n",
    "                'SUBLOCATION', #024\n",
    "                'SUBLOCATION', #025\n",
    "                'SUBLOCATION', #026\n",
    "                'SUBLOCATION', #027\n",
    "                'SUBLOCATION', #028\n",
    "                'SUBLOCATION', #029\n",
    "                '', #030\n",
    "                '', #031\n",
    "                '', #032\n",
    "                'SUBLOCATION', #033\n",
    "                '', #034\n",
    "                'SUBLOCATION', #035\n",
    "                'SUBLOCATION', #036\n",
    "                'SUBLOCATION', #037\n",
    "                'SUBLOCATION', #038\n",
    "                \"\", #039 NEW UPDATE\n",
    "                \"\", #040 NEW UPDATE\n",
    "                \"SUBLOCATION\", #041 NEW UPDATE\n",
    "                \"SUBLOCATION\", #042 NEW UPDATE\n",
    "                \"\", #043 NEW UPDATE\n",
    "                \"\", #044 NEW UPDATE\n",
    "                \"\", #045 NEW UPDATE\n",
    "                \"\", #046 NEW UPDATE\n",
    "                \"SUBLOCATION\", #047 NEW UPDATE\n",
    "                \"SUBLOCATION\", #048 NEW UPDATE\n",
    "                \"SUBLOCATION\", #049 NEW UPDATE\n",
    "                \"SUBLOCATION\", #050 NEW UPDATE\n",
    "                \"SUBLOCATION\", #051 NEW UPDATE\n",
    "                \"\", #052 NEW UPDATE\n",
    "                \"\" #053 NEW UPDATE\n",
    "                ]\n",
    "\n",
    "    # Create fields for \"ASSET_DESC_CAT\" and \"EVENT_DESC_CAT\" # NEW UPDATE FIX\n",
    "    dfInput[[\"ASSET_DESC_CAT\", \"EVENT_DESC_CAT\"]] = dfInput[\"MESSAGE\"].str.split(pat = \": \", expand=True, n = 1)   \n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.strip() # Remove leading and trailing whitespaces\n",
    "    dfInput[\"EVENT_DESC_CAT\"] = dfInput[\"EVENT_DESC_CAT\"].str.strip() # Remove leading and trailing whitespaces\n",
    "    try:\n",
    "        dfInput.loc[df[\"EVENT_DESC_CAT\"].isna(), \"EVENT_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"]\n",
    "        dfInput.loc[df[\"EVENT_DESC_CAT\"] == dfInput[\"ASSET_DESC_CAT\"], \"ASSET_DESC_CAT\"] = np.nan\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Remove Location Names    \n",
    "    try: # Error catch if the entire column is empty\n",
    "        #df= df.replace({\"ASSET_DESC_CAT\": locNames}, regex = True) # not compatible with modin; slower than list method\n",
    "        dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].replace(regex = locNamesList, value = locNamesVal)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # Get Asset Description Category (Remove Numbers)\n",
    "    # Remove Numbers\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(r'\\d+', '', regex = True)\n",
    "\n",
    "    # Account for exceptions to interpolate ASSET_DESC_CAT based on ASSET_DESCRIPTION\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(\" kV\", \"22 kV\", regex = False) # NEW UPDATE FIX\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(\"at KV SW\", \"at 22 kV SW\", regex = False) # NEW UPDATE FIX\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(\"DC  V\", \"DC 1500 V\", regex = False) # NEW UPDATE FIX\n",
    "\n",
    "    ###############################################################\n",
    "    # Get Asset Description Category (Remove Redundant White Spaces)\n",
    "    # Remove redundant white spaces    \n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.strip().str.replace(r'\\s+', ' ', regex = True)  \n",
    "\n",
    "    ###############################################################\n",
    "    # Get Asset Description Category (Account for Misc Exceptions)\n",
    "\n",
    "    # Account for exceptions # NEW UPDATE\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace('SUBLOCATION SUBLOCATION', 'SUBLOCATION', regex = False)\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace('( ', '(', regex = False)\n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.replace(r'\\A(: )','', regex = True)\n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.replace('Cameras','Camera', case = False, regex = False)\n",
    "\n",
    "    try:\n",
    "        dfInput.loc[df[\"MESSAGE\"].str.contains(\"CCTV Controller Power Supply\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"CCTV Controller Power Supply\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"CBN Access Multiplexer\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"CBN Access Multiplexer\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"CI Gas Panel\", case = False, na = False, regex = False), \"ASSET_DESC_CAT\"] = \"CI Gas Panel\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"RI Gas Panel\", case = False, na = False, regex = False), \"ASSET_DESC_CAT\"] = \"CI Gas Panel\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"CROSS-CONNECT ACCESS Multiplexer\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"CROSS-CONNECT ACCESS Multiplexer\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Electrically Supervised Valve\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Electrically Supervised Valve\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Hosereel Pump\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Hosereel Pump\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Level Fire Shutter\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Level Fire Shutter\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Level Roller Shutter\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Level Roller Shutter\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Main Fire Alarm Panel\", na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Main Fire Alarm Panel\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Traffic Direction\", case = False, na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Traffic Direction\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Tunnel LTG Ctrl Panel\", case = False, na = False, regex = False), \"ASSET_DESC_CAT\"] = \"Tunnel LTG Ctrl Panel\"\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dfInput.loc[df[\"ASSET_DESC_CAT\"].str.contains(\"Zone -\", case = False, na = False, regex = False), \"ASSET_DESC_CAT\"] = \"ZONE SUBLOCATION\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Remove additional locations\n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.split(\" at \", n = 1, expand = True)[0]\n",
    "    dfInput[\"ASSET_DESC_CAT\"] = dfInput[\"ASSET_DESC_CAT\"].str.split(\" for \", n = 1, expand = True)[0]\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace('SUBLOCATION-SUBLOCATION', 'SUBLOCATION', regex = False) # New Update\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace('SUBLOCATION-', 'SUBLOCATION', regex = False) # New Update\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace('-SUBLOCATION', 'SUBLOCATION', regex = False) # New Update\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(r'( at)$', '', case = False, regex = True) # New Update\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(r'( for)$', '', case = False, regex = True) # New Update\n",
    "    dfInput['ASSET_DESC_CAT'] = dfInput['ASSET_DESC_CAT'].str.replace(r'^(:)', '', regex = True) # New Update\n",
    "    \n",
    "    # Return dataframe with location names cleaned\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f942ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanEventDesc(dfInput): \n",
    "    ###############################################################\n",
    "\n",
    "    # Create \"EVENT_DESCRIPTION\" field\n",
    "    #df[\"EVENT_DESC_CAT\"] = df[\"MESSAGE\"].copy()  \n",
    "\n",
    "    # Remove Location Names      \n",
    "    try: # Error catch if the entire column is empty\n",
    "        #df = df.replace({\"EVENT_DESC_CAT\": locNames}, regex = True) # Does not work with Modin; slower than list method\n",
    "        dfInput[\"EVENT_DESC_CAT\"] = dfInput[\"EVENT_DESC_CAT\"].replace(regex = locNamesList, value = locNamesVal)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ###############################################################\n",
    "    # Get Event Description Category (Remove Numbers)\n",
    "    # Remove Numbers\n",
    "    dfInput['EVENT_DESC_CAT'] = dfInput['EVENT_DESC_CAT'].str.replace(r'\\d+', '', regex = True)\n",
    "\n",
    "    # Fix entries which still need to preserve number info\n",
    "    dfInput['EVENT_DESC_CAT'] = dfInput['EVENT_DESC_CAT'].str.replace(\" kV\", \"22 kV\", regex = False) # NEW UPDATE FIX\n",
    "    dfInput['EVENT_DESC_CAT'] = dfInput['EVENT_DESC_CAT'].str.replace(\"at KV SW\", \"at 22 kV SW\", regex = False) # NEW UPDATE FIX\n",
    "    dfInput['EVENT_DESC_CAT'] = dfInput['EVENT_DESC_CAT'].str.replace(\"DC  V\", \"DC 1500 V\", regex = False) # NEW UPDATE FIX\n",
    "\n",
    "    try: # New Update\n",
    "        dfInput.loc[dfInput[\"EVENT_DESC_CAT\"] == \"MP  VDC Status\", \"EVENT_DESC_CAT\"] = \"MP 1500 VDC Status\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    ###############################################################\n",
    "    # Get Event Description Category (Remove Redundant White Spaces)\n",
    "    # Remove redundant white spaces    \n",
    "    dfInput[\"EVENT_DESC_CAT\"] = dfInput[\"EVENT_DESC_CAT\"].str.strip().str.replace(r'\\s+', ' ', regex = True)\n",
    "    \n",
    "    ###############################################################\n",
    "    # Get Event Description Category (Account for Misc Exceptions)\n",
    "    # Account for Exceptions\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"logged\", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"Operator\", na = False, regex = False)) &\n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"NelVisu\", na = False, regex = False)),\n",
    "               \"EVENT_DESC_CAT\"] = \"Operator Logged In/Out of NelVisu\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' /, /...', '', regex = False)\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'__:', '', regex = False) \n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'_:', '', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'([.]+){2}', '', regex = True) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'(_+){2}', '', regex = True) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'::', '', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' : ', ': ', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'@n', '', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' ,', ',', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'< >', '', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'-:', ':', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'^(:)', '', regex = True) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'( -)\\S', ' - ', regex = True) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' )', ')', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'()', '', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' ump Rm', ' Pump Rm', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' latform', ' Platform', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r'( )+', ' ', regex = True) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace('SUBLOCATION-SUBLOCATION', 'SUBLOCATION', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace('SUBLOCATION-', 'SUBLOCATION', regex = False) # New Update\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace('-SUBLOCATION', 'SUBLOCATION', regex = False) # New Update\n",
    "    #df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace()\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\" at \", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\": \", na = False, regex = False)), \"EVENT_DESC_CAT\"] = df['EVENT_DESC_CAT'].str.split(\"at\", 1, expand = True)[0] + \": \" + df['EVENT_DESC_CAT'].str.split(\": \", 1, expand = True)[1]\n",
    "        df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.replace(r' : ', ': ', regex = False) # New Update\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Gws\", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"msg in\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = df['EVENT_DESC_CAT'].str.split(\"msg in\", 1, expand = True)[0] + \"msg in SUBLOCATION\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Gws\", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"bcast in\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = df['EVENT_DESC_CAT'].str.split(\"bcast in\", 1, expand = True)[0] + \"bcast in SUBLOCATION\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Train\", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"Car\", na = False, regex = False)) &\n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"assigned\", na = False, regex = False)) &\n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"Manoeuvre\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Manoeuvre assigned to Train Car\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Train\", na = False, regex = False)) & \n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"Car\", na = False, regex = False)) &\n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"abandoned\", na = False, regex = False)) &\n",
    "               (df['EVENT_DESC_CAT'].str.contains(\"Manoeuvre\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Manoeuvre abandoned by Train Car\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['MESSAGE'].str.contains(\"Display of Free-Text\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Display of Free-Text\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"DVA version mismatch\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"DVA version mismatch\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Automatic hand-over\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Automatic hand-over\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Automatic Hold Applied\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Automatic Hold Applied\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # New Update\n",
    "    try:\n",
    "        df.loc[(df['EVENT_DESC_CAT'].str.contains(\"Communication between\", na = False, regex = False)), \"EVENT_DESC_CAT\"] = \"Communication between Nodes\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    df['EVENT_DESC_CAT'] = df['EVENT_DESC_CAT'].str.strip() # New Update\n",
    "    \n",
    "    # Return dataframe Event Description Cleaned\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f417f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTrainInfo(dfInput):\n",
    "    # Extract Train Information\n",
    "    # Get Train ID (# NEW UPDATE)\n",
    "    dfInput[[\"TrainID\", \"TrainID_temp\"]] = dfInput[\"MESSAGE\"].str.extract(r\"TR___(\\d+)|Train (\\d+)[ :]\") \n",
    "    try:\n",
    "        dfInput.loc[dfInput[\"TrainID\"].isna(), \"TrainID\"] =  dfInput[\"TrainID_temp\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Get CarID and ServiceID (NEW UPDATE)\n",
    "    dfInput[[\"CarID\", \"ServiceID\", \"CarID_temp\"]] = dfInput[\"MESSAGE\"].str.extract(r\"cars (\\d+)/(\\d+)|Car (\\d+)[ :]\")\n",
    "    try:\n",
    "        dfInput.loc[df[\"CarID\"].isna(), \"CarID\"] =  dfInputInput[\"CarID_temp\"]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Delete redundant variables\n",
    "    del dfInput[\"TrainID_temp\"], dfInput[\"CarID_temp\"] # NEW UPDATE\n",
    "    \n",
    "    # Return dataframe with train info extracted\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAssetInfo(dfInput):\n",
    "    # Extract Asset Information\n",
    "    dfInput[\"AssetClass\"] = dfInput[\"ASSETNAME\"] \n",
    "\n",
    "    # Remove Location Names  \n",
    "    try: # Error catch if entire column is empty\n",
    "        #df = df.replace({\"AssetClass\": locNames}, regex = True) # Does not work with Modin; slower than list method\n",
    "        dfInput[\"AssetClass\"] = dfInput[\"AssetClass\"].replace(regex = locNamesList, value = \"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Remove Numbers\n",
    "    dfInput['AssetClass'] = dfInput['AssetClass'].str.replace(r'\\d+', '', regex = True)\n",
    "\n",
    "    # Remove Exceptions\n",
    "    try:\n",
    "        dfInput.loc[df['AssetClass'].str.contains(\"TRACTION\", regex = False), 'AssetClass'] = \"TRACTION/TRACTION\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        dfInput.loc[(df['AssetClass'].str.contains(\"TUNNEL\", regex = False)) & \n",
    "           (dfInput['AssetClass'].str.contains(\"LIGHT\", regex = False)), 'AssetClass'] = \"TUNNEL/LIGHT\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Clean up string prior to delimiting\n",
    "    dfInput['AssetClass'] = dfInput['AssetClass'].str.replace(r'\\A(_)', '', regex = True)\n",
    "    dfInput['AssetClass'] = dfInput['AssetClass'].str.replace(r'(_)\\Z', '', regex = True)\n",
    "    dfInput['AssetClass'] = dfInput['AssetClass'].str.replace(r'_+', '/', regex = True)\n",
    "\n",
    "    # Get AssetSubClass\n",
    "    dfInput['AssetSubClass'] = dfInput['AssetClass'].str.extract(r'/(\\w+)$')\n",
    "\n",
    "    # Get AssetClass\n",
    "    dfInput['AssetClass'] = dfInput['AssetClass'].str.extract(r'(\\w+)/')[0]\n",
    "\n",
    "    # Return dataframe with asset info extracted\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e599117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeDType(dfInput):\n",
    "    \n",
    "    # Standardise non-finite values\n",
    "    dfInput = dfInput.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Standardise null values\n",
    "    dfInput = dfInput.replace(\"\", np.nan).fillna(value=np.nan) #NEW UPDATE\n",
    "    \n",
    "    # Fill in null values of key problematic columns with 0\n",
    "    dfInput['FORMAT'] = dfInput['FORMAT'].fillna(0)\n",
    "    dfInput['GROUP1'] = dfInput['GROUP1'].fillna(0)\n",
    "    dfInput['GROUP2'] = dfInput['GROUP2'].fillna(0)\n",
    "    dfInput['TrainID'] = dfInput['TrainID'].fillna(0)\n",
    "    dfInput['CarID'] = dfInput['CarID'].fillna(0)\n",
    "    dfInput['ServiceID'] = dfInput['ServiceID'].fillna(0)\n",
    "    \n",
    "    # Convert data format\n",
    "    dfInput[\"ALARMID\"] = dfInput[\"ALARMID\"].astype(\"int64\")\n",
    "    dfInput[\"VALUE\"] = dfInput[\"VALUE\"].astype(\"int64\")\n",
    "    dfInput['DATEANDTIME'] = pd.to_datetime(df['DATEANDTIME'], dayfirst=True)\n",
    "    dfInput[\"ACKREQUIRED\"] = dfInput[\"ACKREQUIRED\"].astype(\"int64\")\n",
    "    dfInput[\"SEVERITY\"] = dfInput[\"SEVERITY\"].astype(\"int64\")\n",
    "    dfInput[\"EQUIPMENTCLASS\"] = dfInput[\"EQUIPMENTCLASS\"].astype(\"int64\")\n",
    "    dfInput[\"FUNCTIONALCAT\"] = dfInput[\"FUNCTIONALCAT\"].astype(\"int64\")\n",
    "    dfInput[\"GEOGRAPHICALCAT\"] = dfInput[\"GEOGRAPHICALCAT\"].astype(\"int64\")\n",
    "    dfInput[\"GROUP1\"] = dfInput[\"GROUP1\"].astype(\"int64\")\n",
    "    dfInput[\"GROUP2\"] = dfInput[\"GROUP2\"].astype(\"int64\")\n",
    "    dfInput[\"FORMAT\"] = dfInput[\"FORMAT\"].astype(\"int64\")\n",
    "    dfInput[\"TrainID\"] = dfInput[\"TrainID\"].astype(\"int64\")\n",
    "    dfInput[\"CarID\"] = dfInput[\"CarID\"].astype(\"int64\")\n",
    "    dfInput[\"ServiceID\"] = dfInput[\"ServiceID\"].astype(\"int64\")\n",
    "\n",
    "    # Infer datatypes of columns\n",
    "    dfInput = dfInput.infer_objects()\n",
    "    \n",
    "    # Extract out key date and time values for matching with dimension tables\n",
    "    # Get date\n",
    "    dfInput['DATE'] = dfInput['DATEANDTIME'].dt.date\n",
    "    dfInput['DATE'] = pd.to_datetime(dfInput['DATE'])\n",
    "    # Get time of day\n",
    "    dfInput['TIME'] = dfInput['DATEANDTIME'].dt.time\n",
    "    # Get time of day in seconds if it comes in higher resolutions\n",
    "    # Not required\n",
    "    #dfInput['TIME_S'] = dfInput['DATEANDTIME'].dt.floor(\"s\").dt.time\n",
    "\n",
    "    # Return dataframe with data type updated\n",
    "    return dfInput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3cb57d",
   "metadata": {},
   "source": [
    "#### Test dataframe processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataframe processing script\n",
    "if (testMode == True):\n",
    "    # Get list of files to initialise\n",
    "    fileList = os.listdir()\n",
    "    \n",
    "    # Get target file\n",
    "    inputFile = fileList[0]\n",
    "    \n",
    "    # Ingest log files as a dataframe for further processing\n",
    "    df = fileIngestor(inputFile)\n",
    "\n",
    "    # Remove data anomalies\n",
    "    df = removeDataAnomalies(df)\n",
    "\n",
    "    # Clean up location names\n",
    "    df = cleanLocNames(df)\n",
    "\n",
    "    # Clean event description data\n",
    "    df = cleanEventDesc(df)\n",
    "\n",
    "    # Extract train info\n",
    "    df = extractTrainInfo(df)\n",
    "\n",
    "    # Extract asset info\n",
    "    df = extractAssetInfo(df)\n",
    "\n",
    "    # change data type of columns\n",
    "    df = changeDType(df)\n",
    "    \n",
    "    # Inspect data\n",
    "    df.info()\n",
    "    \n",
    "    # Inspect data\n",
    "    print(df.head())\n",
    "    \n",
    "    # Delete data\n",
    "    del df\n",
    "        \n",
    "else:\n",
    "    print(\"Test script skipped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3d0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0160bb",
   "metadata": {},
   "source": [
    "### Process Data (Packaged Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7245d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get timestamp of operation start:\n",
    "dateTimeStartOp = dt.datetime.now()\n",
    "print(\"Start Operation\")\n",
    "print(dateTimeStartOp)\n",
    "\n",
    "# Get list of files to initialise\n",
    "fileList = os.listdir()\n",
    "\n",
    "# Define Archive Folder Location\n",
    "archiveFolderLoc = \"../Logs_archive/\"\n",
    "\n",
    "# Delete or archived completed files\n",
    "delFileCondition = False\n",
    "\n",
    "while len(fileList) > 0:\n",
    "    # Initiate batch run  \n",
    "    print(\"Start batch run\")\n",
    "    # Get timestamp of batch run start:\n",
    "    dateTimeStartBatch = dt.datetime.now()\n",
    "    print(dateTimeStartBatch)\n",
    "    \n",
    "    for file in fileList:\n",
    "        # Get Oldest File in Directory in a First in First Out (FIFO) manner as an input\n",
    "        inputFile = min(fileList, key=os.path.getctime)\n",
    "        \n",
    "        #############################################\n",
    "        # Process file as a cleaned dataframe - START\n",
    "        #############################################\n",
    "        \n",
    "        # Get timestamp of run start:\n",
    "        print(\"Commence processing of\", inputFile)\n",
    "        dateTimeStartRun = dt.datetime.now()\n",
    "        print(dateTimeStartRun)\n",
    "        \n",
    "        # Ingest log files as a dataframe for further processing\n",
    "        df = fileIngestor(inputFile)\n",
    "\n",
    "        # Remove data anomalies\n",
    "        df = removeDataAnomalies(df)\n",
    "\n",
    "        # Clean up location names\n",
    "        df = cleanLocNames(df)\n",
    "\n",
    "        # Clean event description data\n",
    "        df = cleanEventDesc(df)\n",
    "\n",
    "        # Extract train info\n",
    "        df = extractTrainInfo(df)\n",
    "\n",
    "        # Extract asset info\n",
    "        df = extractAssetInfo(df)\n",
    "\n",
    "        # change data type of columns\n",
    "        df = changeDType(df)\n",
    "        \n",
    "        #############################################\n",
    "        # Process file as a cleaned dataframe - END\n",
    "        #############################################\n",
    "        \n",
    "        # Append data to SQL DB\n",
    "        appendData(targetTable, df, serverParams)\n",
    "        \n",
    "        if (delFileCondition == True):\n",
    "            # Delete processed file\n",
    "            os.remove(inputFile)\n",
    "            print(inputFile, \"deleted\")\n",
    "        else:\n",
    "            # Archive processed file\n",
    "            fileTransfer(inputFile, archiveFolderLoc)\n",
    "            #print(inputFile, \"archived\")\n",
    "        \n",
    "        # Update list of files\n",
    "        fileList = os.listdir()\n",
    "        \n",
    "        #Get timestamp of run end: \n",
    "        dateTimeEndRun = dt.datetime.now()\n",
    "        print(\"Run Tlapsed Time\")\n",
    "        print(dateTimeEndRun - dateTimeStartRun)\n",
    "        del dateTimeEndRun, dateTimeStartRun\n",
    "        \n",
    "    print(\"Batch run completed\")\n",
    "    # Get timestamp of batch run end:\n",
    "    dateTimeEndBatch = dt.datetime.now()\n",
    "    print(dateTimeEndBatch)\n",
    "    print(\"Batch Elapsed Time\")\n",
    "    print(dateTimeEndBatch - dateTimeStartBatch)\n",
    "    del dateTimeEndBatch, dateTimeStartBatch\n",
    "    \n",
    "    # When initial batch run is completed check again for newly added files\n",
    "    # This is to account for any lag or latency in the file transfer\n",
    "    # 10 second timer delay between batches\n",
    "    time.sleep(10)    \n",
    "    # Update list of fileshttp://localhost:8888/notebooks/Documents/SBST%20Train%20IAMS%20Project/scripts/HISevent_log_processing_and_storage_v1.1.ipynb#\n",
    "    fileList = os.listdir()\n",
    "    \n",
    "    \n",
    "print(\"Script Terminated\")\n",
    "dateTimeEndOp = dt.datetime.now()\n",
    "print(dateTimeEndOp)\n",
    "print(\"Operation Elapsed Time\")\n",
    "print(dateTimeEndOp - dateTimeStartOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Connection to Database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed84750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a68a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
